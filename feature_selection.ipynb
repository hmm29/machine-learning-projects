{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33929fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Univariate Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from numpy import set_printoptions\n",
    "from pandas import read_csv\n",
    "url = 'https://goo.gl/bDdBiA'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "y = array[:,8]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, y)\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "# summarize selected features\n",
    "features = fit.transform(X)\n",
    "print(features[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Feature selection with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load data\n",
    "url = 'https://goo.gl/bDdBiA'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "RFE = RFE(model, n_features_to_select=3)\n",
    "fit = RFE.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e211d1",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Notice that RFE code block above is only using the fit method (Fit and Multiple Transform idiom). We haven't called transform method yet because we're simply at the feature selection stage. RFE is a method that recursively eliminates features, fit to the training set, to see which ones most influence the output variable (class, in the case of the Logistic Regression model above). Great for reducing overfitting, improving accuracy, and reducing training time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8767be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 5\n",
      "Selected Features: [ True  True  True False False  True  True False]\n",
      "Feature Ranking: [1 1 1 3 4 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Feature selection with RFE\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "RFE = RFE(model, n_features_to_select=5)\n",
    "fit = RFE.fit(X, Y)\n",
    "# print RFE rankings\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d88f58",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Data reduction technique for dimensionality reduction specifically by transforming data into a compressed form.\n",
    "\n",
    "You can choose the number of components in the transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555f8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [ 2.265e-02  9.722e-01  1.419e-01 -5.786e-02 -9.463e-02  4.697e-02\n",
      "   8.168e-04  1.402e-01]\n",
      " [ 2.246e-02 -1.434e-01  9.225e-01  3.070e-01 -2.098e-02  1.324e-01\n",
      "   6.400e-04  1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# PCA for feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X) # note how we only pass X argument\n",
    "# Summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef825cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.109 0.232 0.099 0.082 0.077 0.141 0.118 0.14 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "fit = model.fit(X, Y)\n",
    "print(fit.feature_importances_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "satyrn-python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
